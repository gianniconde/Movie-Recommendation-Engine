{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15363b91",
   "metadata": {},
   "source": [
    "# IST 736 - Text Mining: Movie Recommender\n",
    "By: Gianni Conde, Sana Khan, Sahil Nanavaty\n",
    "Spring 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ba3ec",
   "metadata": {},
   "source": [
    "## Goal\n",
    "To determine movie recommendations based on movie genre and description using supervised and unsupervised machine learning techniques.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5771c693",
   "metadata": {},
   "source": [
    "### Role\n",
    "My role in this analysis was to perform data cleansing and preparation, assist in creating exploratory visualizations, create training and testing data sets, and develope supervised machine learning models. The models that I specifically developed were Multinomial and Binomial Naive Bayes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1856892b",
   "metadata": {},
   "source": [
    "## About the Data\n",
    "The data was retrieved from Kaggle and originated from IMDB.com. The original corpus contained 16 separate CSV files that each represented a genre. These genres included action, adventure, animation, biography, crime, family, fantasy, film-noir, history, horror, mystery, romance, sci-fi, sports, thriller, and war. Each file contained varying amounts of rows and 14 columns. The number of rows between each file ranged from 987 to 52,618. The columns represented movie ID, movie name, year, certificate, runtime, genre, rating, description, director, director ID, star, star ID, votes, and gross in USD.\n",
    "\n",
    "Link(s):\n",
    "https://www.kaggle.com/datasets/rajugc/imdb-movies-dataset-based-on-genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix\n",
    "from sklearn.metrics import (accuracy_score, f1_score, classification_report)\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import random as rd\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "import pyLDAvis\n",
    "from gensim.models import LdaModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e84aebb",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading csv files\n",
    "action = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\action.csv')    \n",
    "adventure = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\adventure.csv')    \n",
    "animation = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\animation.csv')    \n",
    "bio = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\biography.csv')    \n",
    "crime = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\crime.csv')    \n",
    "family = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\family.csv')    \n",
    "fantasy = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\fantasy.csv')    \n",
    "film_noir = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\film-noir.csv')    \n",
    "history = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\history.csv')    \n",
    "horror = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\horror.csv')    \n",
    "mystery = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\mystery.csv')  \n",
    "romance = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\romance.csv')    \n",
    "scifi = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\scifi.csv')    \n",
    "sports = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\sports.csv')    \n",
    "thriller = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\thriller.csv')    \n",
    "war = pd.read_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\data\\war.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda32e3",
   "metadata": {},
   "source": [
    "## Data Cleansing and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee0631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding Genre column\n",
    "action['genre'] = 'action'\n",
    "adventure['genre'] = 'adventure'\n",
    "animation['genre'] = 'animation'\n",
    "bio['genre'] = 'biography'\n",
    "crime['genre'] = 'crime'\n",
    "family['genre'] = 'family'\n",
    "fantasy['genre'] = 'fantasy'\n",
    "film_noir['genre'] = 'film-noir'\n",
    "history['genre'] = 'history'\n",
    "horror['genre'] = 'horror'\n",
    "mystery['genre'] = 'mystery'\n",
    "romance['genre'] = 'romance'\n",
    "scifi['genre'] = 'sci-fi'\n",
    "sports['genre'] = 'sports'\n",
    "thriller['genre'] = 'thriller'\n",
    "war['genre'] = 'war'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da02214",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate all files into on dataframe\n",
    "#movies = pd.concat(file_list, ignore_index = True)\n",
    "movies = pd.concat([action,adventure,animation,bio,crime,family,fantasy,\n",
    "                    film_noir,history,horror,mystery,romance,scifi,\n",
    "                    sports,thriller,war])\n",
    "# Resetting index\n",
    "movies = movies.reset_index(drop=True)\n",
    "movies.head()\n",
    "## Writing the dataframe to a csv file\n",
    "movies.to_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\new data\\combined_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c8eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspection\n",
    "print(movies.shape) # 368,300 rows (movies), 14 columns (categories)\n",
    "print('The column names are:\\n')\n",
    "for col in movies.columns:\n",
    "    print(col)\n",
    "    \n",
    "## Checking for missing values\n",
    "movies.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f47cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Year\n",
    "movies['year'].unique()\n",
    "unwanted_years = ['I', 'II', 'V', 'III', 'VII', 'IV', 'XXIII', 'IX', 'XV', 'VI',\n",
    "                  'X', 'XIV', 'XIX', 'XXIX', 'XXI', 'VIII', 'XI', 'XVIII', 'XII', \n",
    "                  'XIII', 'LXXI', 'XVI', 'XX', 'XXXIII', 'XXXII', 'XXXVI', 'XVII', \n",
    "                  'LXIV', 'LXII', 'LXVIII', 'XL', 'XXXIV', 'XXXI', 'XLV', 'XLIV', \n",
    "                  'XXIV', 'XXVII', 'LX', 'XXV', 'XXXIX', '2029', 'XXVIII', 'XXX', \n",
    "                  'LXXII', '1909', 'XXXVIII', 'XXII', 'LVI', 'LVII' 'XLI', 'LII', \n",
    "                  'XXXVII', 'LIX', 'LVIII', 'LXX', 'XLIII', 'XLIX', 'LXXIV', 'XXVI', \n",
    "                  'C', 'XLI', 'LVII', 'LV','XLVI', 'LXXVII', 'XXXV', 'LIV', 'LI', \n",
    "                  'LXXXII', 'XCIX', 'LXIII']\n",
    "movies_clean = movies[~movies['year'].isin(unwanted_years)]\n",
    "# Filling missing values with 0s\n",
    "movies_clean['year'] = movies_clean['year'].fillna(0).astype(int)\n",
    "# Replacing 0s with NaNs\n",
    "movies_clean['year'] = movies['year'].replace(0, np.nan)\n",
    "# Dropping rows with missing values\n",
    "movies_clean = movies_clean.dropna(subset=['year'])\n",
    "movies_clean['year'].isnull().sum()\n",
    "# Make 'year' column int\n",
    "movies_clean['year'] = movies_clean['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0643d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing unwanted columns\n",
    "movies_clean = movies_clean.drop(['movie_id', 'director_id', 'star_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Director\n",
    "# Change type\n",
    "movies_clean['director'] = movies_clean['director'].str.replace('\\n', '')\n",
    "movies_clean['director'] = movies_clean['director'].fillna('none')\n",
    "\n",
    "## Star\n",
    "# Change type\n",
    "movies_clean['star'] = movies_clean['star'].str.replace('\\n', '')\n",
    "movies_clean['star'] = movies_clean['star'].fillna('none')\n",
    "# Rename column\n",
    "movies_clean.rename(columns = {'star':'actors'}, inplace = True)\n",
    "\n",
    "## Runtime\n",
    "movies_clean = movies_clean.dropna(subset=['runtime'])\n",
    "#Change type to int\n",
    "movies_clean['runtime'] = movies_clean['runtime'].str.replace('min', '')\n",
    "movies_clean['runtime'] = movies_clean['runtime'].str.replace(',', '').astype(int)\n",
    "# Convert to datetime format\n",
    "movies_clean['runtime'] = pd.to_timedelta(movies_clean['runtime'], unit='m')\n",
    "\n",
    "## Certificate\n",
    "movies_clean['certificate'].unique()\n",
    "movies_clean['certificate'].value_counts()\n",
    "movies_clean['certificate'] = movies_clean['certificate'].fillna('NR')\n",
    "movies_clean['certificate'] = movies_clean['certificate'].replace({'Not Rated':'NR'}, regex = True)\n",
    "\n",
    "## Movie Name\n",
    "movies_clean = movies_clean.dropna(subset=['movie_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gross\n",
    "# Rename column\n",
    "movies_clean.rename(columns = {'gross(in $)':'revenue'}, inplace = True)\n",
    "# Normalizing revenue\n",
    "movies_clean['revenue'] = movies_clean['revenue'].fillna(movies_clean['revenue'].median())\n",
    "\n",
    "## Votes\n",
    "movies_clean['votes'] = movies_clean['votes'].fillna(0).astype(int)\n",
    "\n",
    "## Rating\n",
    "movies_clean['rating'] = movies_clean['rating'].fillna(0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d09fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final check for missing values\n",
    "movies_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5cb66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for duplicated rows\n",
    "duplicate_values = movies_clean['movie_name'].duplicated()\n",
    "print(duplicate_values)\n",
    "## Removing duplicate rows\n",
    "movies_clean = movies_clean.drop_duplicates(subset=['movie_name'], keep='first')\n",
    "print(movies_clean)\n",
    "\n",
    "## Writing the clean dataframe to a csv file\n",
    "movies_clean.to_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\new data\\movies_clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b87821",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce542a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_clean.describe()\n",
    "movies_clean.columns.unique()\n",
    "movies_clean.shape # 142,626 movies, 11 columns\n",
    "movies_clean.info()\n",
    "\n",
    "#options for plot styles\n",
    "print(plt.style.available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4acffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_counts = movies_clean['genre'].value_counts()\n",
    "genre_counts\n",
    "\n",
    "## Plotting movies per genre\n",
    "plt.figure(figsize=(10, 6))\n",
    "genre_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Number of Movies per Genre')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.xticks(rotation=45, ha='right')  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d647d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_genre = movies_clean.groupby('genre', as_index=False)['revenue'].sum().sort_values(by='revenue', ascending=False)\n",
    "revenue_genre\n",
    "\n",
    "## Plotting revenue by genre\n",
    "plt.figure(figsize=(14,7))\n",
    "ax = sns.barplot(x=revenue_genre['revenue'], \n",
    "                  y=revenue_genre['genre'], \n",
    "                  palette='crest')\n",
    "plt.xlabel('revenue (hundreds of billions)', fontdict = {'fontname': 'Times New Roman', 'color': 'black', 'fontsize' : '15'})\n",
    "plt.ylabel('genre',fontdict = {'fontname': 'Times New Roman', 'color': 'black', 'fontsize' : '15'})\n",
    "plt.title('Revenue by Genre', \n",
    "          fontdict = {'fontname': 'Times New Roman', 'color': 'black', 'fontsize' : '25'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3429692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average ratings per genre\n",
    "avg_ratings = movies_clean.groupby('genre')['rating'].mean().sort_values(ascending=False).index\n",
    "\n",
    "# Create a boxplot with the specified order\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='Genre', y='rating', data=movies_clean, order=avg_ratings, palette='viridis')\n",
    "plt.title('Boxplot of Ratings per Genre')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Rating')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d1313",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a smaller data frame\n",
    "## Genres: sci-fi, crime, romance, sports, horror\n",
    "movies_clean['genre'].unique()\n",
    "movies_new = movies_clean.loc[movies_clean['genre'].isin(\n",
    "    ['sci-fi','crime','romance','sports','horror'])]\n",
    "movies_new['genre'].unique()\n",
    "movies_new  # 62,010 rows, 11 columns\n",
    "\n",
    "# Writing the new data set to a csv\n",
    "movies_new.to_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\new data\\5_genres.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074c0e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DF only containing title and description\n",
    "movies_new.columns\n",
    "columns_dropped = ['movie_name','year','certificate','runtime','rating', \n",
    "                   'director','actors','votes','revenue']\n",
    "movies_new.drop(columns_dropped, axis = 1, inplace = True)\n",
    "movies_new\n",
    "print(movie_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a48fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movies_new)  # 62,010 descriptions (rows), 2 columns\n",
    "## Reading data as csv, not df\n",
    "movie_description = r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\new data\\5_genres.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb6d528",
   "metadata": {},
   "source": [
    "## Model Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Writing the DF to csv\n",
    "### Sampling the data (30%)\n",
    "myDF = movies_new.sample(frac = 0.3, random_state = 42)\n",
    "print(myDF.shape)  # 18,603 descriptions \n",
    "print(myDF)\n",
    "print(type(myDF))\n",
    "\n",
    "### Writing the DF to a csv\n",
    "myDF.to_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\new data\\descriptions.csv', index = False)\n",
    "infile = r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\new data\\descriptions.csv'\n",
    "print(infile)\n",
    "print(type(infile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd44289",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenize and Vectorize the descriptions\n",
    "## Create the list of descriptions\n",
    "## Keep the labels\n",
    "description_LIST = []\n",
    "label_LIST = []\n",
    "with open(infile, 'r', encoding=\"utf-8\") as FILE:\n",
    "    FILE.readline()\n",
    "    for row in FILE:\n",
    "        next_label, next_description = row.split(',', 1)\n",
    "        label_LIST.append(next_label)\n",
    "        description_LIST.append(next_description)\n",
    "     \n",
    "    FILE.close()\n",
    "    \n",
    "print('The description list is:\\n')\n",
    "print(description_LIST)\n",
    "print('The label list is:\\n')\n",
    "print(label_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3495025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove all words in desciptions that match the genres.\n",
    "new_description_LIST = []\n",
    "\n",
    "for element in description_LIST:\n",
    "    print(element)\n",
    "    print(type(element))\n",
    "    ## make into list\n",
    "    all_words = element.split(\" \")\n",
    "    print(all_words)\n",
    "    ## Now remove words that are in your topics\n",
    "    new_words_LIST = []\n",
    "    for word in all_words:\n",
    "        print(word)\n",
    "        word = word.lower()\n",
    "        if word in infile['description']:\n",
    "            print(word)\n",
    "        else:\n",
    "            new_words_LIST.append(word)            \n",
    "    ##turn back to string\n",
    "    new_words = \" \".join(new_words_LIST)\n",
    "    ## Place into new_headline_LIST\n",
    "    new_description_LIST.append(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1950352",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seting the old description list to the new one\n",
    "description_LIST = new_description_LIST\n",
    "print(description_LIST) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6359a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lemmatizer\n",
    "lemmer = WordNetLemmatizer()\n",
    "def my_lemmer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z\\-]\", \" \", str_input).lower().split()\n",
    "    words = [lemmer.lemmatize(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07951a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instantiating vectorizers\n",
    "## CountVectorizer\n",
    "myCV = CountVectorizer(input = 'content',\n",
    "                       lowercase = True,\n",
    "                       stop_words = 'english',\n",
    "                       max_features = 1000,\n",
    "                       tokenizer = my_lemmer)\n",
    "## Bernoulli\n",
    "myCV2 = TfidfVectorizer(input = 'content',\n",
    "                           stop_words = 'english',\n",
    "                           lowercase = True,\n",
    "                           max_features = 1000,\n",
    "                           binary = True,\n",
    "                           tokenizer = my_lemmer)\n",
    "myCV3 = CountVectorizer(input = 'content',\n",
    "                           stop_words = 'english',\n",
    "                           lowercase = True,\n",
    "                           max_features = 1000,\n",
    "                           binary = True,\n",
    "                           tokenizer = my_lemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8173d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying vectorizers\n",
    "matrix_CV = myCV.fit_transform(description_LIST)\n",
    "matrix_CV2 = myCV2.fit_transform(description_LIST)\n",
    "matrix_CV3 = myCV3.fit_transform(description_LIST)\n",
    "print(type(matrix_CV))\n",
    "print(type(matrix_CV2))\n",
    "print(type(matrix_CV3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieving column names\n",
    "cols1 = myCV.get_feature_names_out()\n",
    "cols2 = myCV2.get_feature_names_out()\n",
    "cols3 = myCV3.get_feature_names_out()\n",
    "print(cols1)\n",
    "print(cols2)\n",
    "print(cols3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ce1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the dataframes\n",
    "DF1 = pd.DataFrame(matrix_CV.toarray(), columns = cols1)\n",
    "DF2 = pd.DataFrame(matrix_CV2.toarray(), columns = cols2)\n",
    "DF3 = pd.DataFrame(matrix_CV3.toarray(), columns = cols3)\n",
    "## Checking columns \n",
    "DF1.columns\n",
    "DF2.columns\n",
    "DF3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d34d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove columns containing numbers\n",
    "def remove_cols(myStrings):\n",
    "    return any(char.isdigit() for char in myStrings)\n",
    "for next_col in DF1.columns:\n",
    "    logical = remove_cols(next_col)\n",
    "    if(logical==True):\n",
    "        DF1 = DF1.drop([next_col], axis = 1)\n",
    "        DF2 = DF2.drop([next_col], axis = 1)\n",
    "        DF3 = DF3.drop([next_col], axis = 1)\n",
    "    ## Remove columns containing 2 words or less\n",
    "    elif(len(str(next_col))<=2):\n",
    "        print(next_col)\n",
    "        DF1 = DF1.drop([next_col], axis = 1)\n",
    "        DF2 = DF2.drop([next_col], axis = 1)\n",
    "        DF3 = DF3.drop([next_col], axis = 1)\n",
    " \n",
    "print(DF1)\n",
    "print(DF2) \n",
    "print(DF3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing \n",
    "DF1 = DF1.drop('-year-old', axis = 1)\n",
    "DF2 = DF2.drop('-year-old', axis = 1)\n",
    "DF2 = DF2.drop('-', axis = 1)\n",
    "DF3 = DF3.drop('-year-old', axis = 1)\n",
    "DF3 = DF3.drop('-', axis = 1)\n",
    "print(DF1)\n",
    "print(DF2)\n",
    "print(DF3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b011fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding labels to the dfs\n",
    "DF1.insert(loc = 0, column = 'GENRE', value = label_LIST)\n",
    "DF2.insert(loc = 0, column = 'GENRE', value = label_LIST)\n",
    "DF3.insert(loc = 0, column = 'GENRE', value = label_LIST)\n",
    "print(DF1) # 18,603 rows, 998 unique words\n",
    "print(DF2) # 18,603 rows, 999 unique words\n",
    "print(DF3)\n",
    "## Writing the new dfs to their own csv files\n",
    "DF1.to_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\new data\\df_CV.csv', index = False)\n",
    "DF2.to_csv(r'C:\\Users\\cassh\\OneDrive\\Desktop\\IST-736\\Project\\new data\\df_TFID.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b3e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the data into training and testing sets\n",
    "train1, test1 = train_test_split(DF1, test_size = 0.3, random_state = 42)\n",
    "train2, test2 = train_test_split(DF2, test_size = 0.3, random_state = 42)\n",
    "train3, test3 = train_test_split(DF3, test_size = 0.3, random_state = 42)\n",
    "print(train1) # 13,022 rows\n",
    "print(test1)  # 5,581 rows\n",
    "print(train2)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving and removing labels DF1\n",
    "train1_LABEL = train1['GENRE']\n",
    "train1_DATA = train1.drop(columns='GENRE')\n",
    "test1_LABEL = test1['GENRE']\n",
    "test1_DATA = test1.drop(columns='GENRE')\n",
    "print(train1_LABEL)\n",
    "print(train1_DATA)\n",
    "print(test1_LABEL)\n",
    "print(test1_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded8c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving and removing labels DF2\n",
    "train2_LABEL = train2['GENRE']\n",
    "train2_DATA = train2.drop(columns='GENRE')\n",
    "test2_LABEL = test2['GENRE']\n",
    "test2_DATA = test2.drop(columns='GENRE')\n",
    "print(train2_LABEL)\n",
    "print(train2_DATA)\n",
    "print(test2_LABEL)\n",
    "print(test2_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113cf23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving and removing labels DF3\n",
    "train3_LABEL = train3['GENRE']\n",
    "train3_DATA = train3.drop(columns='GENRE')\n",
    "test3_LABEL = test3['GENRE']\n",
    "test3_DATA = test3.drop(columns='GENRE')\n",
    "print(train3_LABEL)\n",
    "print(train3_DATA)\n",
    "print(test3_LABEL)\n",
    "print(test3_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89550d9b",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b9272d",
   "metadata": {},
   "source": [
    "### Model 1: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908589d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fitting the Multinomial NB model\n",
    "## Instantiate\n",
    "NB = MultinomialNB()\n",
    "## Model Fitting\n",
    "NB_model1 = NB.fit(train1_DATA, train1_LABEL)\n",
    "print('\\nThe classes are:')\n",
    "print(NB_model1.classes_)  # crime, horror, romance, sci-fi, sports\n",
    "print('\\nThe class counts are:')\n",
    "print(NB_model1.class_count_)  # crime:3488, horror:3128, romance:5457, scifi:526, sports:423\n",
    "print('\\nThe feature log probabilities are:')\n",
    "print(NB_model1.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bba696",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction\n",
    "prediction1 = NB_model1.predict(test1_DATA)\n",
    "print(np.round(NB_model1.predict_proba(test1_DATA), 2))\n",
    "print('\\nThe prediction from NB is:')\n",
    "print(prediction1)\n",
    "print('\\nThe actual labels are:')\n",
    "print(test1_LABEL)\n",
    "print('\\nThe value counts of the actual labels are:')\n",
    "print(test1_LABEL.value_counts())  # crime:2303, horror:1537, romance:1330, scifi:219, sports:192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e969c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Accuracy\n",
    "accuracy1 = accuracy_score(test1_LABEL, prediction1)\n",
    "print('Accuracy of MultinomialNB: {}%'.format(round(accuracy1 * 100, 2))) # accuracy of 66.91%\n",
    "print(classification_report(test1_LABEL, prediction1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd6de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion Matrix\n",
    "cm_NB1 = confusion_matrix(test1_LABEL, prediction1)\n",
    "print(cm_NB1)\n",
    "classes = ['crime','horror','romance','sci-fi','sports'] \n",
    "sns.heatmap(cm_NB1, annot = True, fmt = 'd', cmap = 'flare', \n",
    "            xticklabels = classes, yticklabels = classes)\n",
    "plt.title('Confusion Matrix for MultinomialNB (CV)')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b56bcc",
   "metadata": {},
   "source": [
    "### Model 2: Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88678248",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fitting the Bernoulli NB model (CV)\n",
    "## Instantiate\n",
    "BNB = BernoulliNB()\n",
    "## Model Fitting\n",
    "BNB_model2 = BNB.fit(train3_DATA, train3_LABEL)\n",
    "print('\\nThe classes are:')\n",
    "print(BNB_model2.classes_)  # crime, horror, romance, sci-fi, sports\n",
    "print('\\nThe class counts are:')\n",
    "print(BNB_model2.class_count_)  # crime:3488, horror:3128, romance:5457, scifi:526, sports:423\n",
    "print('\\nThe feature log probabilities are:')\n",
    "print(BNB_model2.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8577b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction\n",
    "prediction4 = BNB_model2.predict(test3_DATA)\n",
    "print(np.round(BNB_model2.predict_proba(test3_DATA), 2))\n",
    "print('\\nThe prediction from NB is:')\n",
    "print(prediction4)\n",
    "print('\\nThe actual labels are:')\n",
    "print(test3_LABEL)\n",
    "print('\\nThe value counts of the actual labels are:')\n",
    "print(test3_LABEL.value_counts())  # crime:2303, horror:1537, romance:1330, scifi:219, sports:192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Accuracy\n",
    "accuracy4 = accuracy_score(test3_LABEL, prediction4)\n",
    "print('Accuracy of MultinomialNB: {}%'.format(round(accuracy4 * 100, 2))) # accuracy of 66.87%\n",
    "print(classification_report(test3_LABEL, prediction4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion Matrix\n",
    "cm_BNB2 = confusion_matrix(test3_LABEL, prediction4)\n",
    "print(cm_BNB2)\n",
    "classes = ['crime','horror','romance','sci-fi','sports'] \n",
    "sns.heatmap(cm_BNB2, annot = True, fmt = 'd', cmap = 'flare_r', \n",
    "            xticklabels = classes, yticklabels = classes)\n",
    "plt.title('Confusion Matrix for BernoulliNB (CV)')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef1a68f",
   "metadata": {},
   "source": [
    "When comparing the results of both Naive Bayes models, it appears that they are both nearly identical. Their similar accuracy score imply that the features are not highly correlated and that the genres were well separated by movie description features. The low classification report scores for sci-fi and sports were most likely due to low support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753244e7",
   "metadata": {},
   "source": [
    "### Model 3: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define kernels and costs\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "costs = [0.1, 1, 10]\n",
    "\n",
    "# Train models\n",
    "results = []\n",
    "for kernel in kernels:\n",
    "    for cost in costs:\n",
    "        model = SVC(kernel=kernel, C=cost)\n",
    "        model.fit(train2_DATA, train2_LABEL)\n",
    "        predictions = model.predict(test2_DATA)\n",
    "        accuracy = accuracy_score(test2_LABEL, predictions)\n",
    "        results.append((kernel, cost, accuracy))\n",
    "        print(f'Kernel: {kernel}, Cost: {cost}, Accuracy: {accuracy}')\n",
    "        \n",
    "# Results\n",
    "for result in results:\n",
    "    print(f'Kernel: {result[0]}, Cost: {result[1]}, Accuracy: {result[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8922a",
   "metadata": {},
   "source": [
    "The linear kernel, while simplistic, was expected to offer the most computationally efficient model. In contrast, the RBF and polynomial kernels were hypothesized to yield higher accuracy scores after precise parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0599c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix    \n",
    "for kernel in kernels:\n",
    "    for cost in costs:\n",
    "        model = SVC(kernel=kernel, C=cost)\n",
    "        model.fit(train2_DATA, train2_LABEL)\n",
    "        predictions = model.predict(test2_DATA)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        results.append((kernel, cost, accuracy))\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        cm = confusion_matrix(test2_DATA, predictions, labels=model.classes_)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(10,7))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=model.classes_, yticklabels=model.classes_)\n",
    "        plt.title(f'Confusion Matrix for Kernel: {kernel}, Cost: {cost}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "        \n",
    "print(f'Kernel: {kernel}, Cost: {cost}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04a192b",
   "metadata": {},
   "source": [
    "Higher values of the cost parameter within the linear kernel allow the model to penalize misclassified points more heavily, leading to a tighter decision boundary. However, the recorded improvements in accuracy were relatively small across different cost values. This suggests that the linear kernel may have reached its performance limit on this dataset. \n",
    "\n",
    "The RBF kernel's accuracy score improved when transitioning from a low-cost value of 0.1 to a higher cost value of 1. This indicates that a moderate cost value was potentially more suitable for this kernel on this dataset. Despite this observation, increasing the cost parameter further to 10 resulted in a decrease in accuracy. This could indicate that the decision boundary became too rigid, leading to overfitting on the training data and reduced generalization.\n",
    "\n",
    "The polynomial kernel displayed a lower accuracy compared to the others across all cost values. Increasing the cost value from 0.1 to 1 led to a significant improvement in accuracy, suggesting that a moderate cost value allowed was more appropriate for the polynomial kernel. \n",
    "\n",
    "Ultimately, increasing the cost parameter to 10 resulted in a decrease in accuracy, similar to the behavior observed with the RBF kernel. This indicates that the decision boundary may have been too complex which led to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422608ab",
   "metadata": {},
   "source": [
    "### Model 4: K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DF1\n",
    "inertia = []\n",
    "k_values = range(1, 11)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow curve\n",
    "plt.plot(k_values, inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c223c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF1 = DF1.drop('LABEL', axis=1)\n",
    "\n",
    "# Instantiate and fit the KMeans model\n",
    "My_KMean = KMeans(n_clusters=3, random_state=42)\n",
    "My_KMean.fit(df_vector1)\n",
    "\n",
    "# Predict clusters\n",
    "My_labels = My_KMean.predict(df_vector1)\n",
    "print(My_labels)\n",
    "\n",
    "My_KMean= KMeans(n_clusters=3)\n",
    "My_KMean.fit(DF1)\n",
    "My_labels=My_KMean.predict(DF1)\n",
    "print(My_labels)\n",
    "\n",
    "My_KMean2 = KMeans(n_clusters=4).fit(preprocessing.normalize(DF1))\n",
    "My_KMean2.fit(DF1)\n",
    "My_labels2=My_KMean2.predict(DF1)\n",
    "print(My_labels2)\n",
    "\n",
    "My_KMean3= KMeans(n_clusters=3)\n",
    "My_KMean3.fit(DF1)\n",
    "My_labels3=My_KMean3.predict(DF1)\n",
    "print(\"Silhouette Score for k = 3 \\n\",silhouette_score(DF1, My_labels3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c8910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cluster labels to the original DataFrame\n",
    "DF1['Cluster_KMeans3'] = My_labels3\n",
    "\n",
    "# Perform PCA for dimensionality reduction to plot in 2D\n",
    "pca = PCA(n_components=2)\n",
    "DF1_pca = pca.fit_transform(DF1)\n",
    "\n",
    "# Plot the data points with color-coded clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Plot the KMeans (k=3) clusters\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=DF1_pca[:, 0], y=DF1_pca[:, 1], hue=DF1['Cluster_KMeans3'], palette='viridis', legend='full')\n",
    "plt.title('KMeans (k=3) Clustering')\n",
    "\n",
    "# Plot the silhouette scores\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x=[silhouette_score(DF1, My_labels3)], y=['KMeans (k=3)'], palette='viridis')\n",
    "plt.xlabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for KMeans (k=3)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b725e",
   "metadata": {},
   "source": [
    "It appears that clusters 0 and 1 have a lot of similarities with a few outliers. However, cluster 2 is spread out and does not share as many similarities amongst the movie genres. If a viewer were to watch a movie from 0 and wanted to watch another movie from this cluster, the recommendation would surely be a good match. Although, given the disparity amongst the last cluster, it may be difficult to provide an accurate recommendation for the viewer. \n",
    "\n",
    "The silhouette score of 0.16 further suggests that while the clusters are distinct enough to be separate, the separation is not particularly strong. This means that the descriptions contain either ambiguous terms or the genres are not distinctly different in the features used for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1216c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans4 = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans4.fit(DF1)\n",
    "my_labels4 = kmeans4.predict(DF1)\n",
    "\n",
    "DF1['Cluster_KMeans4'] = my_labels4\n",
    "\n",
    "# Perform PCA for dimensionality reduction to plot in 2D\n",
    "pca = PCA(n_components=2)\n",
    "DF1_pca = pca.fit_transform(DF1)\n",
    "\n",
    "# Plot the data points with color-coded clusters\n",
    "plt.figure(figsize=(14, 6))\n",
    "# Plot the KMeans (k=4) clusters\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=DF1_pca[:, 0], y=DF1_pca[:, 1], hue=DF1['Cluster_KMeans4'], palette='viridis', legend='full')\n",
    "plt.title('KMeans (k=4) Clustering')\n",
    "\n",
    "# Calculate the silhouette score for k=4 and plot\n",
    "silhouette_avg4 = silhouette_score(DF1, my_labels4)\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x=['KMeans (k=4)'], y=[silhouette_avg4], palette='viridis')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for KMeans (k=4)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3717573d",
   "metadata": {},
   "source": [
    "The model contained 4 clusters as the elbow appears to plateau between 3 and 4. There are improvements in the clusters due to the addition of a new cluster absorbing some of the data points that were found in other clusters. Cluster 1 has some overlap with cluster 3, but is otherwise more defined in comparison to the previous plot. Cluster 3 has a relatively wider spread, indicating that there may not be as many similarities amongst them. Clusters 0 and 2 contain some outliers, but otherwise have a more defined shape which indicates some similarities in the words found in the descriptions. \n",
    "\n",
    "The silhouette for 4 clusters provided an improved score of 0.35. This score suggests that the clusters are reasonably well separated and that the points within each cluster are relatively close to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db07b8",
   "metadata": {},
   "source": [
    "### Model 5: Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d47e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyVectLDA_DH=CountVectorizer(input='content', stop_words= \"english\")\n",
    "\n",
    "Vect_DH = MyVectLDA_DH.fit_transform(DF1)\n",
    "ColumnNamesLDA_DH=MyVectLDA_DH.get_feature_names()\n",
    "CorpusDF_DH=pd.DataFrame(Vect_DH.toarray(),columns=ColumnNamesLDA_DH)\n",
    "print(CorpusDF_DH)\n",
    "\n",
    "lda_model_DH = LatentDirichletAllocation(n_components=5, max_iter=100, learning_method='online')\n",
    "LDA_DH_Model = lda_model_DH.fit_transform(Vect_DH)\n",
    "\n",
    "print(\"SIZE: \", LDA_DH_Model.shape)  # (NO_DOCUMENTS, NO_TOPICS)\n",
    "\n",
    "# Let's see how the first document in the corpus looks like in\n",
    "## different topic spaces\n",
    "print(LDA_DH_Model[0])\n",
    "print(LDA_DH_Model[6])\n",
    "print(\"List of prob: \")\n",
    "print(LDA_DH_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9bb5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_word = 'cluster_kmeans3'  \n",
    "\n",
    "plt.figure(figsize=(num_topics * 5, 8))\n",
    "\n",
    "for t in range(num_topics):\n",
    "    plt.subplot(1, num_topics, t + 1)\n",
    "    plt.ylim(0, num_top_words + 0.5)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title('Topic #{}'.format(t))\n",
    "    top_words_idx = np.argsort(word_topic[:,t])[::-1]  # descending order\n",
    "    top_words_idx = top_words_idx[:num_top_words]\n",
    "    top_words = vocab_array[top_words_idx]\n",
    "    top_words_shares = word_topic[top_words_idx, t]\n",
    "\n",
    "    word_count = 0\n",
    "    for i, (word, share) in enumerate(zip(top_words, top_words_shares)):\n",
    "        if word != exclude_word:  # Only plot the word if it is not the excluded word\n",
    "            plt.text(0.3, num_top_words-word_count-0.5, word, fontsize=fontsize_base)\n",
    "            word_count += 1\n",
    "        if word_count >= num_top_words:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022984d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = gensimvis.prepare(LDA_DH_Model, Vect_DH, vectorizer, mds='tsne')\n",
    "pyLDAvis.display(panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff665f0",
   "metadata": {},
   "source": [
    "The LDA model suggests that it is difficult to identify a clear genre for either topic. Topic 0 could be crime films. Words like suspect, murdered and survive would align best with movies containing criminal elements. Topic 2 could be related to horror movies. Words such as thriller, violent, woods, and unexpected indicate something fearful which is the basis of the horror genre. Topic 3 could potentially be about sports movies, with words like team, race, and time. Topic 4 seems best aligned to sci-fi with zombie vampire, and tale. Topic 1 does not appear to have a clear genre. However, based on words like woman, tragic, and teenager, the topic could be pertaining to romance movies. \n",
    "\n",
    "While LDA is effective at providing a general idea of the top features per topic, it was unfortunately not able to provide definitive genres classifications. This is likely due to the overlap between the text descriptions and the fact that movies can belong to multiple distinct genres. Therefore, LDA may not be the most adequate model to use for a recommendation system. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
